{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-08-11T15:57:51.230644Z","iopub.execute_input":"2021-08-11T15:57:51.231029Z","iopub.status.idle":"2021-08-11T16:00:01.509032Z","shell.execute_reply.started":"2021-08-11T15:57:51.230949Z","shell.execute_reply":"2021-08-11T16:00:01.50799Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Import Libraries**","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport os\nimport cv2\nimport pickle\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport random\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","execution":{"iopub.status.busy":"2021-08-11T16:00:01.510707Z","iopub.execute_input":"2021-08-11T16:00:01.511045Z","iopub.status.idle":"2021-08-11T16:00:08.40813Z","shell.execute_reply.started":"2021-08-11T16:00:01.511013Z","shell.execute_reply":"2021-08-11T16:00:08.40719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Data PreProcessing**","metadata":{}},{"cell_type":"code","source":"labels = ['bad', 'good']\nimg_size = 100\ndef get_training_data(data_dir):\n    data = [] \n    for label in labels: \n        path = os.path.join(data_dir, label)\n        class_num = labels.index(label)\n        for img in os.listdir(path):\n            try:\n                img_arr = cv2.imread(os.path.join(path, img),cv2.IMREAD_GRAYSCALE)\n                resized_arr = cv2.resize(img_arr, (img_size, img_size))\n                data.append([resized_arr, class_num])\n            except Exception as e:\n                print(e)\n    return np.array(data)","metadata":{"execution":{"iopub.status.busy":"2021-08-11T16:00:08.420794Z","iopub.execute_input":"2021-08-11T16:00:08.421078Z","iopub.status.idle":"2021-08-11T16:00:08.428271Z","shell.execute_reply.started":"2021-08-11T16:00:08.421052Z","shell.execute_reply":"2021-08-11T16:00:08.427224Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = get_training_data('../input/cubesat/training_dataset_v3')\ntest = get_training_data('../input/cubesat/test_dataset_v5')","metadata":{"execution":{"iopub.status.busy":"2021-08-11T16:00:08.430649Z","iopub.execute_input":"2021-08-11T16:00:08.430955Z","iopub.status.idle":"2021-08-11T16:02:50.85963Z","shell.execute_reply.started":"2021-08-11T16:00:08.430919Z","shell.execute_reply":"2021-08-11T16:02:50.855302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Data Distribution**","metadata":{}},{"cell_type":"code","source":"bad = 0 \ngood = 0 \n\nfor i, j in train:\n    if j == 0:\n        bad+=1\n    else:\n        good+=1\n        \nprint('Bad:', bad)\nprint('good:', good)\nprint('Bad - Good:', bad-good)\n","metadata":{"execution":{"iopub.status.busy":"2021-08-11T16:02:50.86186Z","iopub.execute_input":"2021-08-11T16:02:50.862147Z","iopub.status.idle":"2021-08-11T16:02:50.94601Z","shell.execute_reply.started":"2021-08-11T16:02:50.862119Z","shell.execute_reply":"2021-08-11T16:02:50.944784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X = []\ny = []\n\nfor feature, label in train:\n    X.append(feature)\n    y.append(label)\n\nfor feature, label in test:\n    X.append(feature)\n    y.append(label)\n    \n\n\n# resize data for deep learning \nX = np.array(X).reshape(-1, img_size, img_size, 1)\ny = np.array(y)\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=32)\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.20, random_state=32)","metadata":{"execution":{"iopub.status.busy":"2021-08-11T16:02:50.947743Z","iopub.execute_input":"2021-08-11T16:02:50.948171Z","iopub.status.idle":"2021-08-11T16:02:51.771484Z","shell.execute_reply.started":"2021-08-11T16:02:50.948125Z","shell.execute_reply":"2021-08-11T16:02:51.770483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Data Reshaping and Normalization**","metadata":{}},{"cell_type":"code","source":"X_train = X_train.reshape(X_train.shape[0],-1).T\nX_test = X_test.reshape(X_test.shape[0],-1).T\nX_train = X_train / 255\nX_test = X_test / 255\nX_val = X_val / 255\ny_train = np.array(y_train).reshape(-1,1)\ny_train = y_train.T\ny_test = np.array(y_test).reshape(-1,1)\ny_test = y_test.T","metadata":{"execution":{"iopub.status.busy":"2021-08-11T16:02:51.772661Z","iopub.execute_input":"2021-08-11T16:02:51.772933Z","iopub.status.idle":"2021-08-11T16:02:53.786009Z","shell.execute_reply.started":"2021-08-11T16:02:51.772898Z","shell.execute_reply":"2021-08-11T16:02:53.784807Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Activation Functions**","metadata":{}},{"cell_type":"markdown","source":"# **Sigmoid**\n<img src = \"https://upload.wikimedia.org/wikipedia/commons/thumb/5/53/Sigmoid-function-2.svg/1280px-Sigmoid-function-2.svg.png\" width=\"400\">","metadata":{}},{"cell_type":"code","source":"def sigmoid(Z):\n    \n    A = 1/(1+np.exp(-Z))\n    cache = Z\n    \n    return A, cache","metadata":{"execution":{"iopub.status.busy":"2021-08-11T16:02:53.787143Z","iopub.execute_input":"2021-08-11T16:02:53.787436Z","iopub.status.idle":"2021-08-11T16:02:53.792804Z","shell.execute_reply.started":"2021-08-11T16:02:53.787408Z","shell.execute_reply":"2021-08-11T16:02:53.791518Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Relu**\n<img src = \"https://sebastianraschka.com/images/faq/relu-derivative/relu_3.png\" width=\"400\">","metadata":{}},{"cell_type":"code","source":"def relu(Z):\n    \n    A = np.maximum(0,Z)\n    \n    assert(A.shape == Z.shape)\n    \n    cache = Z \n    return A, cache","metadata":{"execution":{"iopub.status.busy":"2021-08-11T16:02:53.79437Z","iopub.execute_input":"2021-08-11T16:02:53.794698Z","iopub.status.idle":"2021-08-11T16:02:53.80495Z","shell.execute_reply.started":"2021-08-11T16:02:53.794658Z","shell.execute_reply":"2021-08-11T16:02:53.804148Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Backward Activation Function**","metadata":{}},{"cell_type":"code","source":"def relu_backward(dA, cache):\n\n    \n    Z = cache\n    dZ = np.array(dA, copy=True)\n    \n    dZ[Z <= 0] = 0\n    \n    assert (dZ.shape == Z.shape)\n    \n    return dZ","metadata":{"execution":{"iopub.status.busy":"2021-08-11T16:02:53.806268Z","iopub.execute_input":"2021-08-11T16:02:53.806754Z","iopub.status.idle":"2021-08-11T16:02:53.817807Z","shell.execute_reply.started":"2021-08-11T16:02:53.806707Z","shell.execute_reply":"2021-08-11T16:02:53.816936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def sigmoid_backward(dA, cache):\n\n    \n    Z = cache\n    \n    s = 1/(1+np.exp(-Z))\n    dZ = dA * s * (1-s)\n    \n    assert (dZ.shape == Z.shape)\n    \n    return dZ","metadata":{"execution":{"iopub.status.busy":"2021-08-11T16:02:53.819013Z","iopub.execute_input":"2021-08-11T16:02:53.819383Z","iopub.status.idle":"2021-08-11T16:02:53.832116Z","shell.execute_reply.started":"2021-08-11T16:02:53.819348Z","shell.execute_reply":"2021-08-11T16:02:53.831154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## **Defining the Layer of The Neural Network**\n<img src = \"https://i.imgur.com/UX6M1zX.png\" width = 800>","metadata":{}},{"cell_type":"code","source":"### CONSTANTS DEFINING THE MODEL ####\nn_x = 10000     # num_px * num_px * 1\nn_h = 7\nn_y = 1\nlayers_dims = (n_x, n_h, n_y)","metadata":{"execution":{"iopub.status.busy":"2021-08-11T16:02:53.8333Z","iopub.execute_input":"2021-08-11T16:02:53.833863Z","iopub.status.idle":"2021-08-11T16:02:53.845462Z","shell.execute_reply.started":"2021-08-11T16:02:53.833816Z","shell.execute_reply":"2021-08-11T16:02:53.844314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Implementation of Deep Learning Neural Network Model**\n<img src = \"https://i.imgur.com/cizhNhg.jpeg\" width=\"700\">","metadata":{}},{"cell_type":"markdown","source":"# **Initialize Weights and Bias**","metadata":{}},{"cell_type":"code","source":"def initialize_parameters(layer_dims):\n\n    \n    np.random.seed(3)\n    parameters = {}\n    L = len(layer_dims)            # number of layers in the network\n\n    for l in range(1, L):\n        \n        parameters['W' + str(l)] = np.random.randn(layer_dims[l],layer_dims[l-1])*0.01\n        parameters['b' + str(l)] = np.zeros((layer_dims[l],1))\n        \n        \n        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n\n        \n    return parameters","metadata":{"execution":{"iopub.status.busy":"2021-08-11T16:02:53.84701Z","iopub.execute_input":"2021-08-11T16:02:53.847674Z","iopub.status.idle":"2021-08-11T16:02:53.859201Z","shell.execute_reply.started":"2021-08-11T16:02:53.847623Z","shell.execute_reply":"2021-08-11T16:02:53.858111Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Fordward Propagation**","metadata":{}},{"cell_type":"code","source":"def linear_forward(A, W, b):\n\n    Z = np.dot(W,A)+b\n    \n    \n    assert(Z.shape == (W.shape[0], A.shape[1]))\n    cache = (A, W, b)\n    \n    return Z, cache","metadata":{"execution":{"iopub.status.busy":"2021-08-11T16:02:53.860364Z","iopub.execute_input":"2021-08-11T16:02:53.860744Z","iopub.status.idle":"2021-08-11T16:02:53.874313Z","shell.execute_reply.started":"2021-08-11T16:02:53.86071Z","shell.execute_reply":"2021-08-11T16:02:53.873222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def linear_activation_forward(A_prev, W, b, activation):\n\n    if activation == \"sigmoid\":\n        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n        \n        Z, linear_cache = linear_forward(A_prev,W,b)\n        A, activation_cache = sigmoid(Z)\n        \n    \n    elif activation == \"relu\":\n        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n        \n        Z, linear_cache = linear_forward(A_prev,W,b)\n        A, activation_cache = relu(Z)\n        \n    \n    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n    cache = (linear_cache, activation_cache)\n\n    return A, cache","metadata":{"execution":{"iopub.status.busy":"2021-08-11T16:02:53.875896Z","iopub.execute_input":"2021-08-11T16:02:53.876402Z","iopub.status.idle":"2021-08-11T16:02:53.888124Z","shell.execute_reply.started":"2021-08-11T16:02:53.87635Z","shell.execute_reply":"2021-08-11T16:02:53.88678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Cost Calculation**","metadata":{}},{"cell_type":"markdown","source":"<img src = \"https://www.researchgate.net/profile/Kipp-Johnson/publication/331439931/figure/fig2/AS:733179287728129@1551814972455/mpact-of-deep-learning-design-on-learning-effect-of-learning-rate-A-Efficient.png\" width = \"500\">","metadata":{}},{"cell_type":"code","source":"def compute_cost(AL, Y):\n    \n    m = Y.shape[1]\n\n    # Compute loss from aL and y.\n    \n    cost = -1/m * (np.sum(Y*np.log(AL)+((1-Y)*np.log(1-AL))))\n    \n    \n    cost = np.squeeze(cost)     \n    assert(cost.shape == ())\n    \n    return cost","metadata":{"execution":{"iopub.status.busy":"2021-08-11T16:02:53.889487Z","iopub.execute_input":"2021-08-11T16:02:53.89Z","iopub.status.idle":"2021-08-11T16:02:53.903002Z","shell.execute_reply.started":"2021-08-11T16:02:53.889955Z","shell.execute_reply":"2021-08-11T16:02:53.901779Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Back Propagation**","metadata":{}},{"cell_type":"code","source":"def linear_backward(dZ, cache):\n\n    A_prev, W, b = cache\n    m = A_prev.shape[1]\n\n    \n    dW = 1/m *(np.dot(dZ,A_prev.T))\n    db = 1/m * np.sum(dZ,axis = 1,keepdims = True)\n    dA_prev = np.dot(W.T,dZ)\n    \n    \n    \n    assert (dA_prev.shape == A_prev.shape)\n    assert (dW.shape == W.shape)\n    assert (db.shape == b.shape)\n    \n    return dA_prev, dW, db","metadata":{"execution":{"iopub.status.busy":"2021-08-11T16:02:53.904437Z","iopub.execute_input":"2021-08-11T16:02:53.904911Z","iopub.status.idle":"2021-08-11T16:02:53.917678Z","shell.execute_reply.started":"2021-08-11T16:02:53.904867Z","shell.execute_reply":"2021-08-11T16:02:53.91662Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def linear_activation_backward(dA, cache, activation):\n\n    linear_cache, activation_cache = cache\n    \n    if activation == \"relu\":\n        \n        dZ = relu_backward(dA, activation_cache)\n        dA_prev, dW, db = (linear_backward(dZ,linear_cache))\n        \n        \n    elif activation == \"sigmoid\":\n        \n        dZ = sigmoid_backward(dA, activation_cache)\n        dA_prev, dW, db = (linear_backward(dZ,linear_cache))\n        \n    \n    return dA_prev, dW, db","metadata":{"execution":{"iopub.status.busy":"2021-08-11T16:02:53.919268Z","iopub.execute_input":"2021-08-11T16:02:53.919638Z","iopub.status.idle":"2021-08-11T16:02:53.932228Z","shell.execute_reply.started":"2021-08-11T16:02:53.919592Z","shell.execute_reply":"2021-08-11T16:02:53.931288Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Updating the values of Weights and Bias**","metadata":{}},{"cell_type":"code","source":"def update_parameters(parameters, grads, learning_rate):\n\n    \n    L = len(parameters) // 2 # number of layers in the neural network\n\n    # Update rule for each parameter. Use a for loop.\n    \n    for l in range(L):\n        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)]- learning_rate*grads['dW'+str(l+1)]\n        parameters[\"b\" + str(l+1)] =parameters[\"b\" + str(l+1)] - learning_rate*grads['db'+str(l+1)]\n    \n    return parameters","metadata":{"execution":{"iopub.status.busy":"2021-08-11T16:02:53.936225Z","iopub.execute_input":"2021-08-11T16:02:53.936693Z","iopub.status.idle":"2021-08-11T16:02:53.950552Z","shell.execute_reply.started":"2021-08-11T16:02:53.936651Z","shell.execute_reply":"2021-08-11T16:02:53.949307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**A model For L Layer Deep Neural Network**","metadata":{}},{"cell_type":"code","source":"def L_model_forward(X, parameters):\n\n    caches = []\n    A = X\n    L = len(parameters) // 2                  # number of layers in the neural network\n    \n    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n    for l in range(1, L):\n        A_prev = A \n        A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b' + str(l)], activation = \"relu\")\n        caches.append(cache)\n    \n    # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\n    AL, cache = linear_activation_forward(A, parameters['W' + str(L)], parameters['b' + str(L)], activation = \"sigmoid\")\n    caches.append(cache)\n    \n    assert(AL.shape == (1,X.shape[1]))\n            \n    return AL, caches\n    ","metadata":{"execution":{"iopub.status.busy":"2021-08-11T16:02:53.952148Z","iopub.execute_input":"2021-08-11T16:02:53.952771Z","iopub.status.idle":"2021-08-11T16:02:53.962281Z","shell.execute_reply.started":"2021-08-11T16:02:53.952724Z","shell.execute_reply":"2021-08-11T16:02:53.96079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Predict Function**","metadata":{}},{"cell_type":"code","source":"def predict(X, y, parameters):\n\n    \n    m = X.shape[1]\n    n = len(parameters) // 2 # number of layers in the neural network\n    p = np.zeros((1, m),dtype=int)\n    \n    # Forward propagation\n    probas, caches = L_model_forward(X, parameters)\n\n\n    # convert probas to 0/1 predictions\n    for i in range(0, probas.shape[1]):\n        if probas[0,i] > 0.5:\n            p[0,i] = 1\n        else:\n            p[0,i] = 0\n\n    #print results\n    #print (\"predictions: \" + str(p))\n    #print (\"true labels: \" + str(y))\n    acc = np.sum(p == y)/float(m)\n        \n    return acc","metadata":{"execution":{"iopub.status.busy":"2021-08-11T16:02:53.963856Z","iopub.execute_input":"2021-08-11T16:02:53.964173Z","iopub.status.idle":"2021-08-11T16:02:53.973667Z","shell.execute_reply.started":"2021-08-11T16:02:53.964144Z","shell.execute_reply":"2021-08-11T16:02:53.972555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Finalizing The Model**","metadata":{}},{"cell_type":"code","source":"def two_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 2500, print_cost=False):\n\n    np.random.seed(1)\n    grads = {}\n    costs = []                              # to keep track of the cost\n    m = X.shape[1]                           # number of examples\n    (n_x, n_h, n_y) = layers_dims\n    \n    # Initialize parameters dictionary, by calling one of the functions you'd previously implemented\n    \n    parameters = initialize_parameters(layers_dims)\n    \n    \n    # Get W1, b1, W2 and b2 from the dictionary parameters.\n    W1 = parameters[\"W1\"]\n    b1 = parameters[\"b1\"]\n    W2 = parameters[\"W2\"]\n    b2 = parameters[\"b2\"]\n    \n    # Loop (gradient descent)\n\n    for i in range(0, num_iterations):\n\n        # Forward propagation: LINEAR -> RELU -> LINEAR -> SIGMOID. Inputs: \"X, W1, b1, W2, b2\". Output: \"A1, cache1, A2, cache2\".\n        \n        #minibatches = random_mini_batches(X_train, Y_train, minibatch_size, seed)\n        A1, cache1 = linear_activation_forward(X, W1, b1, 'relu')\n        A2, cache2 = linear_activation_forward(A1,W2,b2,'sigmoid')\n        \n        \n        # Compute cost\n\n        cost = compute_cost(A2, Y)\n        \n        \n        # Initializing backward propagation\n        dA2 = - (np.divide(Y, A2) - np.divide(1 - Y, 1 - A2))\n        \n        # Backward propagation. Inputs: \"dA2, cache2, cache1\". Outputs: \"dA1, dW2, db2; also dA0 (not used), dW1, db1\".\n\n        \n        dA1, dW2, db2 =  linear_activation_backward(dA2, cache2, activation='sigmoid')\n        dA0, dW1, db1 =  linear_activation_backward(dA1, cache1, activation='relu')\n        \n\n        \n        # Set grads['dWl'] to dW1, grads['db1'] to db1, grads['dW2'] to dW2, grads['db2'] to db2\n        grads['dW1'] = dW1\n        grads['db1'] = db1\n        grads['dW2'] = dW2\n        grads['db2'] = db2\n        \n        # Update parameters.\n\n        parameters = update_parameters(parameters, grads, learning_rate)\n\n\n        # Retrieve W1, b1, W2, b2 from parameters\n        W1 = parameters[\"W1\"]\n        b1 = parameters[\"b1\"]\n        W2 = parameters[\"W2\"]\n        b2 = parameters[\"b2\"]\n        \n        # Print the cost every 100 training example\n        if print_cost and i % 100 == 0:\n            print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n        if print_cost and i % 100 == 0:\n            costs.append(cost)\n       \n    # plot the cost\n\n    plt.plot(np.squeeze(costs))\n    plt.ylabel('cost')\n    plt.xlabel('iterations (per hundreds)')\n    plt.title(\"Learning rate =\" + str(learning_rate))\n    plt.show()\n    \n    return parameters","metadata":{"execution":{"iopub.status.busy":"2021-08-11T16:02:53.974879Z","iopub.execute_input":"2021-08-11T16:02:53.97519Z","iopub.status.idle":"2021-08-11T16:02:53.989425Z","shell.execute_reply.started":"2021-08-11T16:02:53.97515Z","shell.execute_reply":"2021-08-11T16:02:53.988388Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Training Model**","metadata":{}},{"cell_type":"code","source":"parameters = two_layer_model(X_train, y_train, layers_dims = (n_x, n_h, n_y), num_iterations = 2500, print_cost=True)\npredictions_train = print(\"train accuracy: {} %\".format(predict(X_train, y_train, parameters) * 100))\npredictions_train = print(\"test accuracy: {} %\".format(predict(X_test, y_test, parameters) * 100))","metadata":{"execution":{"iopub.status.busy":"2021-08-11T16:02:53.990767Z","iopub.execute_input":"2021-08-11T16:02:53.99105Z","iopub.status.idle":"2021-08-11T17:49:02.536407Z","shell.execute_reply.started":"2021-08-11T16:02:53.991021Z","shell.execute_reply":"2021-08-11T17:49:02.53506Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Testing Model**","metadata":{}},{"cell_type":"code","source":"test_image1 = cv2.imread('../input/cubesat/test_dataset_v5/good/0.jpg')\ntest_image = cv2.imread('../input/cubesat/test_dataset_v5/good/14.jpg',cv2.IMREAD_GRAYSCALE)\nresized_arr1 = cv2.resize(test_image, (100, 100))\nresized_arr1 = np.array(resized_arr1).reshape(-1, img_size, img_size, 1)\nresized_arr1 = resized_arr1.reshape(resized_arr1.shape[0],-1).T\nresized_arr1 = resized_arr1/255\nmy_predicted_image = predict(resized_arr1, my_label_y, parameters)\n\nplt.imshow(test_image1)\nprint (\"ACCURACY IS= \" + str(np.squeeze(my_predicted_image)*100 )+\"%\" )\nif my_predicted_image>0.5:\n    print(\"There is Good Image\")\nelse:\n    print(\"The Image is Bad Image\")\n\n","metadata":{"execution":{"iopub.status.busy":"2021-08-11T17:49:02.539628Z","iopub.execute_input":"2021-08-11T17:49:02.540692Z","iopub.status.idle":"2021-08-11T17:49:02.550397Z","shell.execute_reply.started":"2021-08-11T17:49:02.540621Z","shell.execute_reply":"2021-08-11T17:49:02.549238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}